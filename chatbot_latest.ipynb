{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f379f4-60b6-48b4-a791-2509c10eb778",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install argparse\n",
    "pip install openai\n",
    "pip install numpy\n",
    "pip install unstructured[pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ebfa1-5b32-4101-b54e-bb3f03df5564",
   "metadata": {},
   "source": [
    "# All Codes working, First is Speech To Text, Second is General Code without category counts, Last is with Category Counts but without Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7267c33d-5ae6-4988-a8b4-e9863064416d",
   "metadata": {},
   "source": [
    "# Speech to Text with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd6f8e30-aa95-4266-8f27-86218a47f77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Audio:  What exercises are best for women?\n",
      "A combination of aerobic and anaerobic exercises, such as walking, swimming, tennis, or group exercise classes, are recommended for women.\n"
     ]
    }
   ],
   "source": [
    "from deepgram import DeepgramClient, PrerecordedOptions\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# The API key for Deepgram\n",
    "DEEPGRAM_API_KEY = '3aa93addc579bff87f09133448f417528f98bca5'\n",
    "\n",
    "# URL of the audio file to be transcribed\n",
    "AUDIO_URL = {\n",
    "  'url': 'https://firebasestorage.googleapis.com/v0/b/herhealth-9ac3b.appspot.com/o/audio_files%2Faudio_record.3gp?alt=media&token=2b74c9a4-f90c-47a0-a8d6-36cafb477ba4'\n",
    "}\n",
    "\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "openai_api_key = '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'\n",
    "\n",
    "def main():\n",
    "    # Initialize the Deepgram client\n",
    "    deepgram = DeepgramClient(DEEPGRAM_API_KEY)\n",
    "    options = PrerecordedOptions(smart_format=True, model=\"nova-2\", language=\"en-US\")\n",
    "    response = deepgram.listen.prerecorded.v('1').transcribe_url(AUDIO_URL, options)\n",
    "    transcript = response['results']['channels'][0]['alternatives'][0]['transcript']\n",
    "    \n",
    "    # Print the transcribed audio\n",
    "    print(\"Transcribed Audio: \", transcript)\n",
    "    \n",
    "    # Prepare the database for similarity search\n",
    "    embedding_function = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Use the transcript as the query text\n",
    "    query_text = transcript.lower().strip()\n",
    "    \n",
    "    # Search the database for relevant context\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "    if len(results) == 0 or results[0][1] < 0.7:\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Adjust the model as needed\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a personal health companion.\"},\n",
    "            {\"role\": \"user\", \"content\": query_text}\n",
    "            ]\n",
    "        )\n",
    "        response_text = response.choices[0].message.content\n",
    "        print(response_text)\n",
    "    else:        \n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "        prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "        prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "            \n",
    "        model = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "        response_text = model.predict(prompt)\n",
    "    \n",
    "        print(response_text)  # Directly print the response text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e2d97-7365-4829-92b4-64f43ab5c439",
   "metadata": {},
   "source": [
    "# Use this one to get only the response without the additional output (Without Text To Speech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97f594d8-fa85-4073-918f-332479b90ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query (or type 'exit' to quit):  can i eat orange at night\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is generally fine to eat an orange at night. Oranges are a healthy snack choice anytime, as they are a good source of vitamins and minerals. However, for some people, the acidity in oranges may cause discomfort when eaten before bedtime. If you have gastroesophageal reflux disease (GERD) or acid reflux, it's best to avoid acidic foods close to bedtime to prevent symptoms from worsening. Ultimately, it's a personal preference and depends on how your body reacts to oranges at night.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "openai_api_key = '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'\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def main():\n",
    "    # Prepare the DB.\n",
    "    embedding_function = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    while True:\n",
    "        query_text = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        if query_text.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        # Search the DB.\n",
    "        results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "        if len(results) == 0 or results[0][1] < 0.7:\n",
    "            response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Adjust the model as needed\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a personal health companion.\"},\n",
    "                {\"role\": \"user\", \"content\": query_text}\n",
    "                ]\n",
    "            )\n",
    "            response_text = response.choices[0].message.content\n",
    "            print(response_text)\n",
    "        else:        \n",
    "            context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "            prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "            prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "            \n",
    "            model = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "            response_text = model.predict(prompt)\n",
    "    \n",
    "            print(response_text)  # Directly print the response text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd083b1-571a-4eb7-adf5-a4e42f0ada02",
   "metadata": {},
   "source": [
    "# Category Counts with Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abbab102-0fdf-4870-9010-79a8f8cef60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query (or type 'exit' to quit):  what to do about period pain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To help ease period pain, you can try having a warm bath or shower, using a heat pad or hot water bottle on your tummy, massaging your tummy and back, trying gentle exercise like yoga, swimming, walking, or cycling, and taking painkillers like paracetamol or ibuprofen. Additionally, cutting down on alcohol and not smoking may also help. If the pain is severe, a GP may recommend anti-inflammatory medicines like naproxen, flurbiprofen, or mefenamic acid, or a TENS machine.\n",
      "Updated counts for 'Menstruation': 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based on the following context and a follow up question, and rephrase the follow up question to be a standalone question in its original language but dont mention Based on the Context in your answer:\n",
    "\n",
    "{context}\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Answer the question based on the above context and follow up input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "\n",
    "openai_api_key = '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'\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'Data Classification - Sheet1.csv'\n",
    "data_classification = pd.read_csv(csv_file_path)\n",
    "\n",
    "# JSON file path\n",
    "json_file_path = 'query_category_counts.json'\n",
    "\n",
    "# Function to initialize or load the counts JSON file\n",
    "def load_or_initialize_counts(csv_data, json_path):\n",
    "    if Path(json_path).exists():\n",
    "        with open(json_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        # Extract unique categories from the CSV and initialize counts to 0\n",
    "        unique_categories = csv_data['Category'].unique()\n",
    "        counts = {category: 0 for category in unique_categories}\n",
    "        with open(json_path, 'w') as file:\n",
    "            json.dump(counts, file, indent=4)\n",
    "        return counts\n",
    "\n",
    "# Function to update the count for a given document's category\n",
    "def update_category_count(document_name, csv_data, json_path):\n",
    "    # Find the category for the given document\n",
    "    category_series = csv_data[csv_data['Document_Name'] == document_name]['Category']\n",
    "    if not category_series.empty:\n",
    "        category = category_series.iloc[0]\n",
    "        # Load the current counts\n",
    "        counts = load_or_initialize_counts(csv_data, json_path)\n",
    "        # Update the count for the found category\n",
    "        if category in counts:\n",
    "            counts[category] += 1\n",
    "        else:\n",
    "            # This case should not occur since we initialize with all categories\n",
    "            counts[category] = 1\n",
    "        # Save the updated counts\n",
    "        with open(json_path, 'w') as file:\n",
    "            json.dump(counts, file, indent=4)\n",
    "        print(f\"Updated counts for '{category}': {counts[category]}\")\n",
    "    else:\n",
    "        print(f\"No category found for document: {document_name}\")\n",
    "\n",
    "def main():\n",
    "    # Prepare the DB.\n",
    "    embedding_function = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    while True:\n",
    "        query_text = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        if query_text.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        # Search the DB.\n",
    "        results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "        if len(results) == 0 or results[0][1] < 0.7:\n",
    "            response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Adjust the model as needed\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a personal health companion.\"},\n",
    "                {\"role\": \"user\", \"content\": query_text}\n",
    "                ]\n",
    "            )\n",
    "            response_text = response.choices[0].message.content\n",
    "            print(response_text)\n",
    "        else:        \n",
    "            context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "            prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "            prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "            \n",
    "            sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "            \n",
    "            model = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "            response_text = model.predict(prompt)\n",
    "    \n",
    "            print(response_text)  # Directly print the response text\n",
    "            update_category_count(sources[0], data_classification, json_file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e83e5e1-e08b-4858-82c9-ffe35424c41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate==0.27.2Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "aiofiles==23.2.1\n",
      "aiohttp==3.9.3\n",
      "aiosignal==1.3.1\n",
      "annotated-types==0.6.0\n",
      "antlr4-python3-runtime==4.9.3\n",
      "anyio==3.7.1\n",
      "argon2-cffi==23.1.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.3.0\n",
      "asgiref==3.7.2\n",
      "asttokens==2.4.1\n",
      "async-lru==2.0.4\n",
      "asyncer==0.0.2\n",
      "attrs==23.2.0\n",
      "Babel==2.14.0\n",
      "backoff==2.2.1\n",
      "bcrypt==4.1.2\n",
      "beautifulsoup4==4.12.3\n",
      "bidict==0.23.1\n",
      "bitsandbytes==0.42.0\n",
      "bleach==6.1.0\n",
      "build==1.1.1\n",
      "cachetools==5.3.3\n",
      "certifi==2024.2.2\n",
      "cffi==1.16.0\n",
      "chainlit==1.0.301\n",
      "chardet==5.2.0\n",
      "charset-normalizer==3.3.2\n",
      "chroma-hnswlib==0.7.3\n",
      "chromadb==0.4.24\n",
      "click==8.1.7\n",
      "colorama==0.4.6\n",
      "coloredlogs==15.0.1\n",
      "comm==0.2.1\n",
      "contourpy==1.2.0\n",
      "cryptography==42.0.2\n",
      "ctransformers==0.2.27\n",
      "cycler==0.12.1\n",
      "dataclasses-json==0.6.4\n",
      "dataclasses-json-speakeasy==0.5.11\n",
      "debugpy==1.8.1\n",
      "decorator==5.1.1\n",
      "deepgram-sdk==3.1.7\n",
      "defusedxml==0.7.1\n",
      "Deprecated==1.2.14\n",
      "distro==1.9.0\n",
      "effdet==0.4.1\n",
      "emoji==2.10.1\n",
      "executing==2.0.1\n",
      "faiss-cpu==1.8.0\n",
      "fastapi==0.108.0\n",
      "fastapi-socketio==0.0.10\n",
      "fastjsonschema==2.19.1\n",
      "filelock==3.13.1\n",
      "filetype==1.2.0\n",
      "flatbuffers==23.5.26\n",
      "fonttools==4.49.0\n",
      "fqdn==1.5.1\n",
      "frozenlist==1.4.1\n",
      "fsspec==2024.2.0\n",
      "google-auth==2.28.2\n",
      "googleapis-common-protos==1.62.0\n",
      "greenlet==3.0.3\n",
      "grpcio==1.62.0\n",
      "h11==0.14.0\n",
      "httpcore==1.0.4\n",
      "httptools==0.6.1\n",
      "httpx==0.27.0\n",
      "huggingface-hub==0.20.3\n",
      "humanfriendly==10.0\n",
      "idna==3.6\n",
      "importlib-metadata==6.11.0\n",
      "importlib-resources==6.1.1\n",
      "iopath==0.1.10\n",
      "ipykernel==6.29.3\n",
      "ipython==8.22.1\n",
      "ipywidgets==8.1.2\n",
      "isoduration==20.11.0\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.3\n",
      "joblib==1.3.2\n",
      "json5==0.9.18\n",
      "jsonpatch==1.33\n",
      "jsonpath-python==1.0.6\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.21.1\n",
      "jsonschema-specifications==2023.12.1\n",
      "jupyter==1.0.0\n",
      "jupyter-console==6.6.3\n",
      "jupyter-events==0.9.0\n",
      "jupyter-lsp==2.2.3\n",
      "jupyter_client==8.6.0\n",
      "jupyter_core==5.7.1\n",
      "jupyter_server==2.12.5\n",
      "jupyter_server_terminals==0.5.2\n",
      "jupyterlab==4.1.2\n",
      "jupyterlab_pygments==0.3.0\n",
      "jupyterlab_server==2.25.3\n",
      "jupyterlab_widgets==3.0.10\n",
      "kiwisolver==1.4.5\n",
      "kubernetes==29.0.0\n",
      "langchain==0.1.10\n",
      "langchain-community==0.0.25\n",
      "langchain-core==0.1.28\n",
      "langchain-text-splitters==0.0.1\n",
      "langdetect==1.0.9\n",
      "langsmith==0.1.13\n",
      "layoutparser==0.3.4\n",
      "Lazify==0.4.0\n",
      "literalai==0.0.204\n",
      "lxml==5.1.0\n",
      "MarkupSafe==2.1.5\n",
      "marshmallow==3.20.2\n",
      "matplotlib==3.7.2\n",
      "matplotlib-inline==0.1.6\n",
      "mistune==3.0.2\n",
      "mmh3==4.1.0\n",
      "monotonic==1.6\n",
      "mpmath==1.3.0\n",
      "multidict==6.0.5\n",
      "mypy-extensions==1.0.0\n",
      "nbclient==0.9.0\n",
      "nbconvert==7.16.1\n",
      "nbformat==5.9.2\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.2.1\n",
      "nltk==3.8.1\n",
      "notebook==7.1.1\n",
      "notebook_shim==0.2.4\n",
      "numpy==1.26.4\n",
      "oauthlib==3.2.2\n",
      "omegaconf==2.3.0\n",
      "onnx==1.15.0\n",
      "onnxruntime==1.15.1\n",
      "openai==1.13.3\n",
      "opencv-python==4.8.0.76\n",
      "opentelemetry-api==1.23.0\n",
      "opentelemetry-exporter-otlp==1.23.0\n",
      "opentelemetry-exporter-otlp-proto-common==1.23.0\n",
      "opentelemetry-exporter-otlp-proto-grpc==1.23.0\n",
      "opentelemetry-exporter-otlp-proto-http==1.23.0\n",
      "opentelemetry-instrumentation==0.44b0\n",
      "opentelemetry-instrumentation-asgi==0.44b0\n",
      "opentelemetry-instrumentation-fastapi==0.44b0\n",
      "opentelemetry-proto==1.23.0\n",
      "opentelemetry-sdk==1.23.0\n",
      "opentelemetry-semantic-conventions==0.44b0\n",
      "opentelemetry-util-http==0.44b0\n",
      "orjson==3.9.15\n",
      "overrides==7.7.0\n",
      "packaging==23.2\n",
      "pandas==2.2.0\n",
      "pandocfilters==1.5.1\n",
      "parso==0.8.3\n",
      "pdf2image==1.17.0\n",
      "pdfminer==20191125\n",
      "pdfminer.six==20221105\n",
      "pdfplumber==0.10.4\n",
      "pikepdf==8.11.0\n",
      "pillow==10.2.0\n",
      "pillow_heif==0.15.0\n",
      "platformdirs==4.2.0\n",
      "portalocker==2.8.2\n",
      "posthog==3.5.0\n",
      "prometheus_client==0.20.0\n",
      "prompt-toolkit==3.0.43\n",
      "protobuf==4.23.4\n",
      "psutil==5.9.8\n",
      "pulsar-client==3.4.0\n",
      "pure-eval==0.2.2\n",
      "py-cpuinfo==9.0.0\n",
      "pyasn1==0.5.1\n",
      "pyasn1-modules==0.3.0\n",
      "pycocotools==2.0.7\n",
      "pycparser==2.21\n",
      "pycryptodome==3.20.0\n",
      "pydantic==2.6.3\n",
      "pydantic_core==2.16.3\n",
      "Pygments==2.17.2\n",
      "PyJWT==2.8.0\n",
      "PyMuPDF==1.23.26\n",
      "PyMuPDFb==1.23.22\n",
      "pyparsing==3.0.9\n",
      "pypdf==4.0.1\n",
      "PyPDF2==3.0.1\n",
      "pypdfium2==4.27.0\n",
      "PyPika==0.48.9\n",
      "pyproject_hooks==1.0.0\n",
      "pyreadline3==3.4.1\n",
      "pytesseract==0.3.10\n",
      "python-dateutil==2.8.2\n",
      "python-dotenv==1.0.1\n",
      "python-engineio==4.9.0\n",
      "python-graphql-client==0.4.3\n",
      "python-iso639==2024.2.7\n",
      "python-json-logger==2.0.7\n",
      "python-magic==0.4.27\n",
      "python-multipart==0.0.9\n",
      "python-socketio==5.11.1\n",
      "pytz==2024.1\n",
      "pywin32==306\n",
      "pywinpty==2.0.13\n",
      "PyYAML==6.0.1\n",
      "pyzmq==25.1.2\n",
      "qtconsole==5.5.1\n",
      "QtPy==2.4.1\n",
      "rapidfuzz==3.6.1\n",
      "referencing==0.33.0\n",
      "regex==2023.12.25\n",
      "requests==2.31.0\n",
      "requests-oauthlib==1.4.0\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rpds-py==0.18.0\n",
      "rsa==4.9\n",
      "safetensors==0.3.2\n",
      "scikit-learn==1.4.1.post1\n",
      "scipy==1.10.1\n",
      "Send2Trash==1.8.2\n",
      "sentence-transformers==2.5.1\n",
      "simple-websocket==1.0.0\n",
      "six==1.16.0\n",
      "sniffio==1.3.1\n",
      "soupsieve==2.5\n",
      "SQLAlchemy==2.0.27\n",
      "stack-data==0.6.3\n",
      "starlette==0.32.0.post1\n",
      "sympy==1.12\n",
      "syncer==2.0.3\n",
      "tabulate==0.9.0\n",
      "tenacity==8.2.3\n",
      "terminado==0.18.0\n",
      "threadpoolctl==3.3.0\n",
      "tiktoken==0.6.0\n",
      "timm==0.9.12\n",
      "tinycss2==1.2.1\n",
      "tokenizers==0.15.2\n",
      "tomli==2.0.1\n",
      "torch==2.2.0\n",
      "torchvision==0.17.0\n",
      "tornado==6.4\n",
      "tqdm==4.66.2\n",
      "traitlets==5.14.1\n",
      "transformers==4.37.1\n",
      "typer==0.9.0\n",
      "types-python-dateutil==2.8.19.20240106\n",
      "typing-inspect==0.9.0\n",
      "typing_extensions==4.9.0\n",
      "tzdata==2024.1\n",
      "unstructured==0.12.6\n",
      "unstructured-client==0.18.0\n",
      "unstructured-inference==0.7.23\n",
      "unstructured.pytesseract==0.3.12\n",
      "uptrace==1.22.0\n",
      "uri-template==1.3.0\n",
      "urllib3==1.26.18\n",
      "uvicorn==0.25.0\n",
      "verboselogs==1.7\n",
      "watchfiles==0.20.0\n",
      "wcwidth==0.2.13\n",
      "webcolors==1.13\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.7.0\n",
      "websockets==12.0\n",
      "widgetsnbextension==4.0.10\n",
      "wrapt==1.16.0\n",
      "wsproto==1.2.0\n",
      "yarl==1.9.4\n",
      "zipp==3.17.0\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da96f88-a591-4c46-887c-e914ec51d4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ebfc0a8d552866b0d59eba665220a57de3bc06f3ac643b8bef38dd8f66781fdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
